{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e247c989",
   "metadata": {},
   "source": [
    "## Analyze Descriptive Passages\n",
    "\n",
    "* setup\n",
    "* content\n",
    "* parts of speech based\n",
    "* time series\n",
    "* topic modeling\n",
    "* neural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e7c71",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5f7a431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# POS\n",
    "import spacy\n",
    "\n",
    "# nltk for wordnet and tokenization\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import WordNetError\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e60b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Read in .csv of descriptive passages as a Pandas data frame\n",
    "Add appropriate header to the columns as well.\n",
    "col names are: \n",
    "[blank],passage,book,left_claim,left_claim_keywords,\n",
    "right_claim,right_claim_keywords,claim_id,passage_id,\n",
    "passage_size,match_output\n",
    "\n",
    "'''\n",
    "def read_as_df(filename):\n",
    "    # read data\n",
    "    df = pd.read_csv(filename)\n",
    "    # filter out first column as well as books that are not Left/Both\n",
    "    df = df[df['match_output'] != 'Right']\n",
    "    # drop unneeded row number\n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5e0062b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Helper for reporting 5-number summary of an inputted list\n",
    "'''\n",
    "def five_number(data):\n",
    "    # calculate quartiles\n",
    "    quartiles = np.percentile(data, [25, 50, 75])\n",
    "    # calculate min/max\n",
    "    data_min, data_max = data.min(), data.max()\n",
    "    # print 5-number summary\n",
    "    print('Min: %.3f' % data_min)\n",
    "    print('Q1: %.3f' % quartiles[0])\n",
    "    print('Median: %.3f' % quartiles[1])\n",
    "    print('Q3: %.3f' % quartiles[2])\n",
    "    print('Max: %.3f' % data_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40aab4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_df = read_as_df('data/descriptive_claims_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a970d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2383, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1df333de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['passage', 'book', 'left_claim', 'left_claim_keywords', 'right_claim',\n",
       "       'right_claim_keywords', 'claim_id', 'passage_id', 'passage_size',\n",
       "       'match_output'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a83ead4",
   "metadata": {},
   "source": [
    "### content work\n",
    "* descriptive words / total words (quite pessimistic)\n",
    "* words per unique thing (Tenen) -- in just these descriptive passages; aka Unique Clutter Distance\n",
    "* words per thing (Tenen) -- in just these descriptive passages (self-selecting sample); aka Clutter Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39acd2d",
   "metadata": {},
   "source": [
    "### Parts of Speech Based Analysis\n",
    "\n",
    "* spaCy on each description\n",
    "    * general counts of adj, prep, pronoun, and can be used for later analysis\n",
    "* column view a la Bal, Tenen\n",
    "* specificity (Nelson 2020)\n",
    "    * per descriptive passage, calculate specificity rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af438a",
   "metadata": {},
   "source": [
    "#### Column View (Bal, Tenen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "70461794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun phrase\n",
    "\n",
    "# grammatical category (typed dependency)\n",
    "\n",
    "# wordnet super sense\n",
    "\n",
    "def noun_inventory(sample, passage_id):\n",
    "    tagged_sample=nlp(sample)\n",
    "    nouns = []\n",
    "    grammatical_categories = []\n",
    "    supersenses = []\n",
    "    for word in tagged_sample:\n",
    "        if word.pos_ == \"NOUN\":\n",
    "            synset = word.lemma_ + \".\" + \"n\" + \".01\"    \n",
    "            # occasionally, an adjective will be misidentified as a noun;\n",
    "            # in that case, catch the exception (since the word won't a true synset)\n",
    "            try: \n",
    "                synset_def = wn.synset(synset)\n",
    "            except WordNetError as w_e:\n",
    "                continue\n",
    "            else:\n",
    "                # see https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L364\n",
    "                # store the lexicographer filename - https://wordnet.princeton.edu/documentation/lexnames5wn\n",
    "                lex_name = synset_def._lexname\n",
    "                # store the word\n",
    "                nouns.append(word)\n",
    "                # store the dependency parse\n",
    "                # see https://downloads.cs.stanford.edu/nlp/software/dependencies_manual.pdf\n",
    "                grammatical_categories.append(word.dep_)\n",
    "                supersenses.append(lex_name)\n",
    "    return nouns, grammatical_categories, supersenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ba550c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The : det\n",
      "cat : nsubj\n",
      "jumped : ROOT\n",
      "over : prep\n",
      "the : det\n",
      "dog : pobj\n",
      ". : punct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([cat, dog], ['nsubj', 'pobj'], ['noun.animal', 'noun.animal'])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_inventory(\"The cat jumped over the dog.\", 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93914577",
   "metadata": {},
   "source": [
    "#### Specificity (Nelson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b407a90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fbb1c920700>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "nlp.remove_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02ad75f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function from http://www.nltk.org/howto/wordnet.html to get *all* of a synset's hyponym/hypernyms\n",
    "hyper = lambda s: s.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2cd48ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Consult wordnet for the situation of a noun and verb with respect to its station in the hypernym hierarchy. \n",
    "Based on current SOA, it is acceptable to simply grab the top-level (.01) synset.\n",
    "\n",
    "Args:\n",
    "    tagged_sample: a spacy doc\n",
    "\n",
    "Return:\n",
    "    specificity: a value conveying the \"specificity\" of the input, via Nelson (2020)\n",
    "'''\n",
    "\n",
    "def specificity(sample, passage_id):\n",
    "    tagged_sample=nlp(sample)\n",
    "    hyper_sum = 0\n",
    "    noun_and_verb_count = 0\n",
    "    for word in tagged_sample:\n",
    "        if word.pos_ == \"NOUN\" or word.pos_ == \"VERB\":\n",
    "            noun_and_verb_count +=1\n",
    "            # if it's a verb, get the most common verb hypernym chain\n",
    "            # else, get the most common noun hypernym chain\n",
    "            pos = word.pos_\n",
    "            tag = \"n\" if pos.startswith(\"N\") else \"v\"\n",
    "            synset = word.lemma_ + \".\" + tag + \".01\"    \n",
    "            # occasionally, an adjective will be misidentified as a noun;\n",
    "            # in that case, catch the exception (since the word won't a true synset) + fix the count\n",
    "            try: \n",
    "                synset_def = wn.synset(synset)\n",
    "            except WordNetError as w_e:\n",
    "                noun_and_verb_count -= 1\n",
    "                continue\n",
    "            else:\n",
    "                hyper_sum += len(list(synset_def.closure(hyper)))\n",
    "\n",
    "    # a few 'descriptive' passages lack \n",
    "    if noun_and_verb_count == 0:\n",
    "        return 0\n",
    "    return hyper_sum / noun_and_verb_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "768d4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity_scores = descriptive_df.apply(lambda x: specificity(x.passage, x.passage_id), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bd7d64b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.000\n",
      "Q1: 3.444\n",
      "Median: 4.333\n",
      "Q3: 5.200\n",
      "Max: 10.667\n"
     ]
    }
   ],
   "source": [
    "five_number(specificity_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c5f3b",
   "metadata": {},
   "source": [
    "### time series\n",
    "\n",
    "would need:\n",
    "* number of fragments total\n",
    "* number of descriptive fragments\n",
    "* publish years for each work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c77850",
   "metadata": {},
   "source": [
    "### topic model\n",
    "\n",
    "* what is each description/claim talking about"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea20361",
   "metadata": {},
   "source": [
    "### embeddings.. \n",
    "* universal sentence encoder, across each description, and then cluster together?\n",
    "* looking for different authors creating similar descriptions ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
