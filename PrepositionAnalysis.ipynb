{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a4b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter\n",
    "from collections import Counter\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# POS\n",
    "import spacy\n",
    "\n",
    "# sorting dicts\n",
    "import operator\n",
    "\n",
    "# chi squared\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import fisher_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d94a4834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1366f1d60>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner,parser'])\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d0e9a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Read in .tsv of tagged sample data as a Pandas data frame\n",
    "Add appropriate header to the columns as well.\n",
    "'''\n",
    "def read_as_df(filename):\n",
    "    \n",
    "    df = pd.read_csv(filename, sep=\"\\t\", header = None)\n",
    "    df.columns = [\"Text_Loc\", \"Sample\", \"Rating\", \"Specificity\",\"Adj\", \"Adv\", \"Noun\", \"Verb\", \"Adp\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a011308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Identifies frequencies for preposition usage across the samples.\n",
    "Provides output in the form suitable for chi-square analysis.\n",
    "'''\n",
    "def prep_capture(samples, detail_rating):\n",
    "    preps = set()\n",
    "    details_counter = Counter()\n",
    "    not_details_counter = Counter()\n",
    "    total_words = 0\n",
    "    detail_word_count = 0\n",
    "    not_detail_word_count = 0\n",
    "    for index, value in enumerate(samples):\n",
    "        score = detail_rating[index]\n",
    "        sample = nlp(samples[index])\n",
    "        for word in sample:\n",
    "            if word.pos_ != 'PUNC':\n",
    "                total_words += 1\n",
    "                if score >= 3.0:\n",
    "                    detail_word_count += 1\n",
    "                else:\n",
    "                    not_detail_word_count += 1\n",
    "            if word.pos_ == 'ADP':\n",
    "                if score >= 3.0:\n",
    "                    if word.text not in details_counter:\n",
    "                        details_counter[word.text] = 1\n",
    "                    else:\n",
    "                        details_counter[word.text] += 1\n",
    "                    \n",
    "                else:\n",
    "                    if word.text not in not_details_counter:\n",
    "                        not_details_counter[word.text] = 1\n",
    "                    else:\n",
    "                        not_details_counter[word.text] += 1\n",
    "    \n",
    "    # first, remove anything that has an observed count of < 5 -- probably some weird typo.\n",
    "    \n",
    "    details_counter = {k: v for k, v in details_counter.items() if v >= 5}\n",
    "    not_details_counter = {k: v for k, v in not_details_counter.items() if v >= 5}\n",
    "    \n",
    "    # now, double check that details_counter and not_details_counter both have same elements\n",
    "    # if they aren't, add the missing entries and give them a count of 0\n",
    "    \n",
    "    for key in details_counter.keys():\n",
    "        if key not in not_details_counter.keys():\n",
    "            not_details_counter[key] = 0\n",
    "    for key in not_details_counter.keys():\n",
    "        if key not in details_counter.keys():\n",
    "            details_counter[key] = 0\n",
    "            \n",
    "    # sort them, by their keys, so we can easily iterate + compare.\n",
    "    sorted_detail_counts = dict(sorted(details_counter.items(), key=operator.itemgetter(0), reverse = False))\n",
    "    sorted_not_detail_counts = dict(sorted(not_details_counter.items(), key=operator.itemgetter(0), reverse = False))\n",
    "\n",
    "    return sorted_detail_counts, sorted_not_detail_counts, total_words, detail_word_count, not_detail_word_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4c4fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_test(detail_counts, not_detail_counts, total_word_count, detail_word_count, not_detail_word_count):\n",
    "    \n",
    "    # write output to a file.\n",
    "    data = []\n",
    "    for word, count in detail_counts.items():\n",
    "        # top left\n",
    "        word_of_interest_detail_count = detail_counts[word]\n",
    "        # bottom left\n",
    "        word_of_interest_not_detail_count = not_detail_counts[word]\n",
    "        \n",
    "        # top right\n",
    "        detail_leftover = detail_word_count - word_of_interest_detail_count\n",
    "        # bottom right\n",
    "        not_detail_leftover = not_detail_word_count - word_of_interest_not_detail_count\n",
    "        \n",
    "        # now, transform each of these \"cells\" into the frequencies expected by chi squared.\n",
    "        table = [[word_of_interest_detail_count, detail_leftover],\n",
    "                [word_of_interest_not_detail_count, not_detail_leftover]]\n",
    "        \n",
    "        # run chi square\n",
    "        stat, p, dof, expected = chi2_contingency(table)\n",
    "        \n",
    "        fisher = False\n",
    "        for row in expected:\n",
    "            for i in row:\n",
    "                if i < 5.0:\n",
    "                    stat, p = fisher_exact(table)\n",
    "                    fisher = True\n",
    "                    \n",
    "        reject = 'REJECT' if p < .05 else 'ACCEPT'\n",
    "        \n",
    "        word_data = []\n",
    "        \n",
    "        word_data.append((word, word_of_interest_detail_count, word_of_interest_not_detail_count, stat, p, reject, fisher))\n",
    "        \n",
    "        data.append(word_data)\n",
    "        \n",
    "    df = pd.DataFrame(data, columns=['word', 'detail_count', 'not_detail_count', 'stat', 'p', 'reject', 'fisher'])\n",
    "        \n",
    "    pd.write_csv(df)\n",
    "        # make sure expected for the table is over 5, or else re-run w/ fisher's exact test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac31f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_as_df(\"data/samples_data.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fcec3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_Loc</th>\n",
       "      <th>Sample</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Adj</th>\n",
       "      <th>Adv</th>\n",
       "      <th>Noun</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Adp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../Gutenberg/samples/heart_of_darkness_20960_2...</td>\n",
       "      <td>...lap. she wore a starched white affair on he...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.565217</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../Gutenberg/samples/adam_bede_601425_602225.txt</td>\n",
       "      <td>...him, if he had known it, that the general a...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.758621</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../Gutenberg/samples/middlemarch_1718514_17193...</td>\n",
       "      <td>...passionate exclamation, as if some torture ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.945455</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../Gutenberg/samples/tom_jones_1740540_1741340...</td>\n",
       "      <td>...so vicious a passion from your heart, and y...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.823529</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../Gutenberg/samples/the_jungle_691598_692398.txt</td>\n",
       "      <td>...intensity, staring at the platform as if no...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.407407</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Text_Loc  \\\n",
       "0  ../Gutenberg/samples/heart_of_darkness_20960_2...   \n",
       "1   ../Gutenberg/samples/adam_bede_601425_602225.txt   \n",
       "2  ../Gutenberg/samples/middlemarch_1718514_17193...   \n",
       "3  ../Gutenberg/samples/tom_jones_1740540_1741340...   \n",
       "4  ../Gutenberg/samples/the_jungle_691598_692398.txt   \n",
       "\n",
       "                                              Sample  Rating  Specificity  \\\n",
       "0  ...lap. she wore a starched white affair on he...     4.5     4.565217   \n",
       "1  ...him, if he had known it, that the general a...     2.5     4.758621   \n",
       "2  ...passionate exclamation, as if some torture ...     2.0     2.945455   \n",
       "3  ...so vicious a passion from your heart, and y...     1.5     4.823529   \n",
       "4  ...intensity, staring at the platform as if no...     2.0     3.407407   \n",
       "\n",
       "   Adj  Adv  Noun  Verb  Adp  \n",
       "0   20    7    29    21   21  \n",
       "1   16   10    36    24   20  \n",
       "2    9    8    21    35   20  \n",
       "3   10   10    29    29   16  \n",
       "4   12   13    22    33    9  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5b287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "detail_preps, not_detail_preps, total_word_count, detail_word_count, not_detail_word_count = prep_capture(df['Sample'], df['Rating'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a222b99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "not_detail_word_count + detail_word_count == total_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d42a9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared_data = chi_square_test(detail_preps, not_detail_preps, total_word_count, detail_word_count, not_detail_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de1c8cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('about', 50, 42, 0.9577494866310962, 0.3277545450719886, 'ACCEPT', False)],\n",
       " [('above', 15, 0, 8.448165359911389, 0.003654118757087796, 'REJECT', False)],\n",
       " [('across', 9, 0, inf, 0.013785158741037891, 'REJECT', True)],\n",
       " [('after',\n",
       "   38,\n",
       "   24,\n",
       "   0.009158609719237133,\n",
       "   0.9237583441122709,\n",
       "   'ACCEPT',\n",
       "   False)],\n",
       " [('against',\n",
       "   17,\n",
       "   12,\n",
       "   0.0025153793545150335,\n",
       "   0.9600000204953151,\n",
       "   'ACCEPT',\n",
       "   False)],\n",
       " [('along', 15, 0, 8.448165359911389, 0.003654118757087796, 'REJECT', False)],\n",
       " [('among', 14, 13, 0.43011242816266726, 0.511933786618215, 'ACCEPT', False)],\n",
       " [('around', 8, 0, inf, 0.02521392390885113, 'REJECT', True)],\n",
       " [('as', 203, 134, 0.005672810396611961, 0.9399616512549686, 'ACCEPT', False)],\n",
       " [('at', 239, 116, 7.907850012440252, 0.004922073325194035, 'REJECT', False)],\n",
       " [('before', 47, 24, 0.929403903766011, 0.3350178682249272, 'ACCEPT', False)],\n",
       " [('behind', 12, 0, inf, 0.002455012450743315, 'REJECT', True)],\n",
       " [('beside', 7, 0, inf, 0.04675106132229724, 'REJECT', True)],\n",
       " [('between',\n",
       "   16,\n",
       "   7,\n",
       "   0.5391678904029764,\n",
       "   0.46277778434594863,\n",
       "   'ACCEPT',\n",
       "   False)],\n",
       " [('beyond', 8, 0, inf, 0.02521392390885113, 'REJECT', True)],\n",
       " [('by', 134, 71, 2.344921603594894, 0.12569188480936394, 'ACCEPT', False)],\n",
       " [('down', 47, 11, 9.943001992395448, 0.0016146202205057452, 'REJECT', False)],\n",
       " [('during', 8, 8, 0.30475829269749694, 0.5809146616780605, 'ACCEPT', False)],\n",
       " [('for', 211, 191, 8.91704035231372, 0.002825219915804036, 'REJECT', False)],\n",
       " [('from', 113, 67, 0.5124894062810413, 0.47406319236849215, 'ACCEPT', False)],\n",
       " [('in', 535, 299, 6.20655288805506, 0.012727824092617242, 'REJECT', False)],\n",
       " [('into', 87, 33, 7.442944300539575, 0.006368561824842411, 'REJECT', False)],\n",
       " [('like', 44, 25, 0.2862274328993415, 0.5926482850177996, 'ACCEPT', False)],\n",
       " [('of',\n",
       "   1017,\n",
       "   568,\n",
       "   12.188625802377388,\n",
       "   0.0004808179071573432,\n",
       "   'REJECT',\n",
       "   False)],\n",
       " [('off', 24, 18, 0.042311533588726295, 0.8370270746393167, 'ACCEPT', False)],\n",
       " [('on', 223, 116, 4.683889870935499, 0.03044673070170938, 'REJECT', False)],\n",
       " [('out', 61, 34, 0.5706314404525966, 0.45000810783323275, 'ACCEPT', False)],\n",
       " [('over', 38, 22, 0.17050655670197332, 0.6796620065980288, 'ACCEPT', False)],\n",
       " [('past', 5, 0, inf, 0.0874129289997089, 'ACCEPT', True)],\n",
       " [('through',\n",
       "   35,\n",
       "   10,\n",
       "   5.277142716848886,\n",
       "   0.021607178506023023,\n",
       "   'REJECT',\n",
       "   False)],\n",
       " [('to', 331, 254, 2.5512975486446585, 0.11020371491785455, 'ACCEPT', False)],\n",
       " [('towards', 14, 15, 1.180977913904193, 0.2771570016277768, 'ACCEPT', False)],\n",
       " [('under', 21, 12, 0.06851825149331796, 0.7935065473138261, 'ACCEPT', False)],\n",
       " [('until', 10, 5, 0.07411849862628211, 0.785432007033074, 'ACCEPT', False)],\n",
       " [('up', 84, 30, 8.482044810734017, 0.0035866872798446963, 'REJECT', False)],\n",
       " [('with',\n",
       "   326,\n",
       "   127,\n",
       "   27.19458619935868,\n",
       "   1.8397311185480371e-07,\n",
       "   'REJECT',\n",
       "   False)],\n",
       " [('within', 13, 0, 7.118759576910055, 0.007628146637715663, 'REJECT', False)],\n",
       " [('without',\n",
       "   32,\n",
       "   21,\n",
       "   0.00456141654351116,\n",
       "   0.946153204588158,\n",
       "   'ACCEPT',\n",
       "   False)]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_squared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b478224",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.append((1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b19ac0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d120a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
