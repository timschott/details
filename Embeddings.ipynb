{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9fb99d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv\n",
    "import csv\n",
    "# off the shelf BERT from Huggingface\n",
    "from transformers import BertModel, BertTokenizer\n",
    "# numpy\n",
    "import numpy as np\n",
    "# pandas\n",
    "import pandas as pd\n",
    "# sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4c4d902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b348dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Read in .tsv of tagged sample data as a Pandas data frame\n",
    "Add appropriate header to the columns as well.\n",
    "'''\n",
    "def read_as_df(filename):\n",
    "    df = pd.read_csv(filename, sep=\"\\t\", header = None, skiprows=[1])\n",
    "    df.columns = [\"Text_Loc\", \"Sample\", \"Rating\", \"Specificity\",\"Adj\", \"Adv\", \"Noun\", \"Verb\", \"Adp\", \"Time\"]\n",
    "    df = df.iloc[1: , :]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2d58e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Helper that invokes https://huggingface.co/transformers/fast_tokenizers.html\n",
    "'''\n",
    "def tokenize_sample(sample):\n",
    "    # tokenize\n",
    "    return tokenizer(sample, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "951404a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Helper to add extra information to a 768-dimension BERT embedding.\n",
    "Could be used in future\n",
    "No application in this notebook since these embeddings are just for a base-truth classification\n",
    "'''\n",
    "def augment_bert_embedding(bert_output, nums_to_add):\n",
    "    temp = np.array(nums_to_add)\n",
    "    augmented_bert_output = np.resize(bert_output, len(bert_output) + len(nums_to_add))\n",
    "    augmented_bert_output[-len(nums_to_add):] = temp\n",
    "    return augmented_bert_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb57194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Extract the embedding values for each token in the samples\n",
    "Uses the word_piece tokenization strategy (as BERT is wont to do).\n",
    "Outputs 1 (num_words, 768) entry in a list per sample\n",
    "'''\n",
    "def get_embeddings(df):\n",
    "    \n",
    "    mat_list = []\n",
    "    for i in range (1, len(df) - 1):\n",
    "        # get sample, remove leading + trailing ellipses\n",
    "        sample = df['Sample'][i][3:-3]\n",
    "        \n",
    "        token_inputs = tokenize_sample(sample)\n",
    "\n",
    "        # converts input_ids to their tokenized form\n",
    "        # ie \"insinuating\" -> is word-pieced into in/##sin/##uating (3 diff tokens!)\n",
    "        tokens=tokenizer.convert_ids_to_tokens(token_inputs[\"input_ids\"][0])\n",
    "        outputs = model(**token_inputs)\n",
    "        embedding_list = []\n",
    "\n",
    "        # need to make a matrix for each sample\n",
    "        for index, token in enumerate(tokens):\n",
    "            # find the rep\n",
    "            bert_embedding = outputs.last_hidden_state[0][index].detach().numpy()\n",
    "            embedding_list.append(bert_embedding)\n",
    "        mat = np.stack(embedding_list, axis=0)\n",
    "        mat_list.append(mat)\n",
    "    return mat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af06e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Also track which tokens get sent to which ids\n",
    "Note that each representation is \"bidirectional\" (which informs the value its embedding takes)\n",
    "But stable per word; that is, \"he\" maps to embedding ID 2002 across every sample\n",
    "So, can reliably keep track of tokens to ids using a dict\n",
    "'''\n",
    "def get_embedding_token_ids(df):\n",
    "    \n",
    "    token_word_mapping = {}\n",
    "    count = 0\n",
    "    for i in range (1, len(df) - 1):\n",
    "        count += 1\n",
    "        sample = df['Sample'][i][3:-3]\n",
    "        token_inputs = tokenize_sample(sample)\n",
    "        tokens=tokenizer.convert_ids_to_tokens(token_inputs[\"input_ids\"][0])\n",
    "        token_ids = np.array(token_inputs[\"input_ids\"][0])\n",
    "        for index, value in enumerate(token_ids.tolist()):\n",
    "            if value not in token_word_mapping:\n",
    "                token_word_mapping[value] = tokens[index]\n",
    "    \n",
    "    return token_word_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05cd1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = read_as_df(\"data/samples_data.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fbbd6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_Loc</th>\n",
       "      <th>Sample</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Adj</th>\n",
       "      <th>Adv</th>\n",
       "      <th>Noun</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Adp</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../Gutenberg/samples/adam_bede_601425_602225.txt</td>\n",
       "      <td>...him, if he had known it, that the general a...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.758620689655173</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../Gutenberg/samples/middlemarch_1718514_17193...</td>\n",
       "      <td>...passionate exclamation, as if some torture ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.9454545454545453</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../Gutenberg/samples/tom_jones_1740540_1741340...</td>\n",
       "      <td>...so vicious a passion from your heart, and y...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.823529411764706</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../Gutenberg/samples/the_jungle_691598_692398.txt</td>\n",
       "      <td>...intensity, staring at the platform as if no...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.4074074074074074</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>../Gutenberg/samples/frankenstein_7967_8767.txt</td>\n",
       "      <td>...tastes are like my own, to approve or amend...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.428571428571429</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Text_Loc  \\\n",
       "1   ../Gutenberg/samples/adam_bede_601425_602225.txt   \n",
       "2  ../Gutenberg/samples/middlemarch_1718514_17193...   \n",
       "3  ../Gutenberg/samples/tom_jones_1740540_1741340...   \n",
       "4  ../Gutenberg/samples/the_jungle_691598_692398.txt   \n",
       "5    ../Gutenberg/samples/frankenstein_7967_8767.txt   \n",
       "\n",
       "                                              Sample Rating  \\\n",
       "1  ...him, if he had known it, that the general a...    2.5   \n",
       "2  ...passionate exclamation, as if some torture ...    2.0   \n",
       "3  ...so vicious a passion from your heart, and y...    1.5   \n",
       "4  ...intensity, staring at the platform as if no...    2.0   \n",
       "5  ...tastes are like my own, to approve or amend...    1.8   \n",
       "\n",
       "          Specificity Adj Adv Noun Verb Adp Time  \n",
       "1   4.758620689655173  16  10   36   24  20    1  \n",
       "2  2.9454545454545453   9   8   21   35  20    6  \n",
       "3   4.823529411764706  10  10   29   29  16    7  \n",
       "4  3.4074074074074074  12  13   22   33   9   11  \n",
       "5   4.428571428571429  18   9   25   25  19    9  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a683eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of matrices of embeddings\n",
    "sample_embeddings = get_embeddings(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3e55605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 768)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of \"tokens\" in the first sample\n",
    "sample_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c0ca003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up dict for id <-> token\n",
    "sample_id_token_mapping = get_embedding_token_ids(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9baa130c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value for he\n",
    "sample_id_token_mapping[2002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1dea33d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7367"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of \"tokens\" across the sampes\n",
    "len(sample_id_token_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1eb88e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "with open('data/sample_embeddings.csv', 'w') as f:\n",
    "    csvwriter = csv.writer(f)\n",
    "    csvwriter.writerows(sample_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3c909040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "with open('data/sample_id_token_mappings.txt', 'w') as f:\n",
    "    print(sample_id_token_mapping, file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
