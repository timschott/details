{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fdb273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re\n",
    "import re\n",
    "\n",
    "# POS\n",
    "import spacy\n",
    "\n",
    "# nltk for wordnet\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2ccfb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x145ca01c0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner,parser'])\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93099db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reads in a file containing text samples, that has each sample separated by a carriage return and then tabs\n",
    "\n",
    "Args:\n",
    "    filename: location of sample file\n",
    "\n",
    "Return:\n",
    "    tagged_samples: a list of spacy `nlp` docs that contains information about each word in the sample\n",
    "'''\n",
    "def read_and_tag(filename):    \n",
    "    \n",
    "    tagged_samples = []\n",
    "    \n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        # samples are separated by line breaks\n",
    "        next(file)\n",
    "        for sample in file:\n",
    "            sample_data = sample.split(\"\\t\")\n",
    "            doc=nlp(sample_data[2])\n",
    "            tagged_samples.append(doc)\n",
    "        \n",
    "    return tagged_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01590ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from http://www.nltk.org/howto/wordnet.html to get *all* of a synset's hyponym/hypernyms\n",
    "hyper = lambda s: s.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f713416",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Consult wordnet for the situation of a verb or noun with respect to its station \n",
    "In the hypernym hierarchy. \n",
    "Based on current SOA, it is acceptable to simply grab the top-level (.01) synset.\n",
    "\n",
    "Args:\n",
    "    tagged_sample: a spacy doc\n",
    "\n",
    "Return:\n",
    "    specificity: a value conveying the \"specificity\" of the input, via Nelson (2020)\n",
    "'''\n",
    "\n",
    "def specificity(tagged_sample):\n",
    "    hyper_sum = 0\n",
    "    noun_and_verb_count = 0\n",
    "    for word in tagged_sample:\n",
    "        if word.pos_ == \"NOUN\" or word.pos_ == \"VERB\":\n",
    "            # if it's a verb, get the most common verb hypernym chain\n",
    "            # else, get the most common noun hypernym chain\n",
    "            pos = word.pos_\n",
    "            tag = \"n\" if pos.startswith(\"N\") else \"v\"\n",
    "            wn_lookup = word.lemma_ + \".\" + tag + \".01\"\n",
    "            try:\n",
    "                hyper_sum += len(list(wn.synset(wn_lookup).closure(hyper)))\n",
    "            except:\n",
    "                # on off chance we have a mistag, don't break down the system\n",
    "                continue\n",
    "            noun_and_verb_count +=1\n",
    "    \n",
    "    return hyper_sum / noun_and_verb_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "769a2e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tally up part of speech tags for categories of interest\n",
    "Args:\n",
    "    tagged_sample: a spacy doc\n",
    "\n",
    "Return:\n",
    "    a tab delimited string containing counts for verbs, adjectives, adverbs, nouns and adpositions (prepositions/postpositions)\n",
    "'''\n",
    "def pos_counts(tagged_sample):\n",
    "    verb_count = 0\n",
    "    adj_count = 0\n",
    "    adv_count = 0\n",
    "    noun_count = 0\n",
    "    adp_count = 0\n",
    "    \n",
    "    for word in tagged_sample:\n",
    "        if word.pos_ == \"NOUN\":\n",
    "            noun_count +=1\n",
    "        elif word.pos_ == \"VERB\":\n",
    "            verb_count +=1\n",
    "        elif word.pos_ == \"ADJ\":\n",
    "            adj_count +=1\n",
    "        elif word.pos_ == \"ADV\":\n",
    "            adv_count +=1\n",
    "        elif word.pos_ == \"ADP\":\n",
    "            adp_count +=1\n",
    "    \n",
    "    return str(adj_count) + \"\\t\" + str(adv_count) + \"\\t\" + str(noun_count) + \"\\t\" + str(verb_count) + \"\\t\" + str(adp_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7804ea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/anlp/lib/python3.8/site-packages/nltk/corpus/reader/wordnet.py:580: UserWarning: Discarded redundant search for Synset('structure.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(wn.synset(\"house.n.01\").closure(hyper)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e45c4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Concatenate data from this notebook and previous surveys.\n",
    "Write the data to an output file.\n",
    "Args:\n",
    "    filename: a file holding text samples, their source and ratings\n",
    "    attr: a list containing the part of speech tag tallies from `pos_counts`\n",
    "    output: the file to write the output to (should be a .tsv)\n",
    "\n",
    "'''\n",
    "def collect_data(filename, attr, output):\n",
    "    row_data = []\n",
    "    \n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        # samples are separated by line breaks\n",
    "        next(file)\n",
    "        for sample in file:\n",
    "            sample_data = sample.split(\"\\t\")\n",
    "            row_data.append(sample_data[1] + \"\\t\" + sample_data[2] + \"\\t\" + sample_data[3])\n",
    "        \n",
    "    for index, val in enumerate(row_data):\n",
    "        row_data[index] = (row_data[index] + \"\\t\" + attr[index]).replace(\"\\n\\t\", \"\\t\")\n",
    "    \n",
    "    f = open(output, \"w\")\n",
    "    for val in row_data:\n",
    "        f.write(val)\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "120934d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_samples = read_and_tag(\"tagged_details_sep_fix.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "da4ff492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10\\t4\\t37\\t22\\t23'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = nlp(\"...century, had been a gigantic pine, with its roots and trunk in the darksome shade, and its head aloft in the upper atmosphere. it was a little dell where they had seated themselves, with a leaf-strewn bank rising gently on either side, and a brook flowing through the midst, over a bed of fallen and drowned leaves. the trees impending over it had flung down great branches from time to time, which choked up the current, and compelled it to form eddies and black depths at some points; while, in its swifter and livelier passages there appeared a channel-way of pebbles, and brown, sparkling sand. letting the eyes follow along the course of the stream, they could catch the reflected light from its water, at some short distance within the forest, but soon lost all traces of it amid the...\")\n",
    "pos_counts(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "02013354",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = []\n",
    "for sample in tagged_samples:\n",
    "    attr.append(str(specificity(sample)) + \"\\t\" + pos_counts(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cddd9af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_data(\"tagged_details_sep_fix.tsv\", attr, \"samples_data_with_spec_and_pos_two.tsv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
