{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "762a99c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re\n",
    "import re\n",
    "\n",
    "from gut_tokenize import read_data, preprocess\n",
    "\n",
    "# sentence tokenize / parse\n",
    "import spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "# enforce max length\n",
    "en.max_length = 1500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a805d451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['robinson_crusoe_clean.txt', 'great_gatsby_clean.txt', 'around_the_world_in_80_days_clean.txt', 'germinal-clean.txt', 'pride_and_prejudice_clean.txt', 'the_jungle_clean.txt', 'scarlet_letter_clean.txt', 'great_expectations_clean.txt', 'moby-clean.txt', 'anna_karenina_clean.txt', 'twenty_thousand_leagues_under_the_sea_clean.txt', 'alice_in_wonderland_clean.txt', 'frankenstein_clean.txt', 'sense_and_sensibility_clean.txt', 'middlemarch_clean.txt', 'tom_jones_clean.txt', 'utopia_clean.txt', 'the_time_machine_clean.txt', 'north_and_south_clean.txt', 'the_fortune_of_the_rougons_clean.txt', 'adam_bede_clean.txt', 'babbit_clean.txt', 'the_last_man_clean.txt', 'madame_bovary_clean.txt', 'picture_of_dorian_grey_clean.txt', 'heart_of_darkness_clean.txt', 'crime_and_punishment_clean.txt', 'bleak_house_clean.txt']\n"
     ]
    }
   ],
   "source": [
    "directory = \"../Gutenberg/cleaned_texts/\"\n",
    "\n",
    "titles, texts = read_data(directory)\n",
    "\n",
    "print(titles)\n",
    "\n",
    "# germinal = preprocess(texts[0])\n",
    "\n",
    "# germinal_clauses = clause_parse(germinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1922473",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This method relies on the spacy parser and returns a clause-level tokenization of a given text.\n",
    "Where \"clause\" is simply text between two delimiters like commas.\n",
    "E.g. the sentence\n",
    "\"this is a test, of many things, but mostly your abilities.\" would split into:\n",
    "\n",
    "- this is a test\n",
    "- of many things\n",
    "- but mostly your abilities\n",
    "\n",
    "See https://stackoverflow.com/a/65300589 for germ of method logic\n",
    "\n",
    "Args:\n",
    "    text: document-level rep of text after being read in and roughly-cleaned\n",
    " \n",
    "Returns:\n",
    "    texts: cleaned. a list holding each clause for the text\n",
    "'''\n",
    "def clause_parse(text):\n",
    "    doc = en(text)\n",
    "    \n",
    "    # keep track of covered words\n",
    "    seen = set()\n",
    "\n",
    "    chunks = []\n",
    "    for sent in doc.sents:\n",
    "        heads = [cc for cc in sent.root.children if cc.dep_ == 'conj']\n",
    "\n",
    "        for head in heads:\n",
    "            words = [ww for ww in head.subtree]\n",
    "            for word in words:\n",
    "                seen.add(word)\n",
    "            chunk = (' '.join([ww.text for ww in words]))\n",
    "            chunks.append((head.i, chunk))\n",
    "\n",
    "        unseen = [ww for ww in sent if ww not in seen]\n",
    "        chunk = ' '.join([ww.text for ww in unseen])\n",
    "        chunks.append((sent.root.i, chunk))\n",
    "\n",
    "    chunks = sorted(chunks, key=lambda x: x[0])\n",
    "\n",
    "    cleaned = []\n",
    "\n",
    "    for ii, chunk in chunks:\n",
    "        # replace boundary char\n",
    "        chunk = chunk.replace(\",\", \"SEP\")\n",
    "        chunk = chunk.replace(\".\", \"SEP\")\n",
    "        # also replace orphan \"and\" sections\n",
    "        chunk = [c.strip() for c in chunk.split(\"SEP\") if c.strip() not in (\"\", \"and\", None)]\n",
    "        cleaned.append(chunk)\n",
    "    \n",
    "    clauses = sum(cleaned, [])\n",
    "    \n",
    "    return [clause for clause in clauses if clause != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c6abbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " over the open plain, beneath a starless sky as dark and thick as ink, a man walked alone along the highway from marchiennes to montsou, a straight paved road ten kilometres in length, intersecting the beetroot fields. he could not even see the black soil before him, and only felt the immense flat horizon by the gusts of march wind, squalls as strong as on the sea, and frozen from sweeping leagues of marsh and naked earth. no tree could be seen against the sky, and the road unrolled as straight as a pier in the midst of the blinding spray of darkness. \n",
      "============\n",
      "over the open plain\n",
      "beneath a starless sky as dark and thick as ink\n",
      "a man walked alone along the highway from marchiennes to montsou\n",
      "a straight paved road ten kilometres in length\n",
      "intersecting the beetroot fields\n",
      "he could not even see the black soil before him\n",
      "only felt the immense flat horizon by the gusts of march wind\n",
      "squalls as strong as on the sea\n",
      "and frozen from sweeping leagues of marsh and naked earth\n",
      "no tree could be seen against the sky\n",
      "the road unrolled as straight as a pier in the midst of the blinding spray of darkness\n"
     ]
    }
   ],
   "source": [
    "print(germinal[0: 558])\n",
    "\n",
    "print('============')\n",
    "\n",
    "clause_count = 0\n",
    "for c in germinal_clauses:\n",
    "    print (c)\n",
    "    clause_count += 1\n",
    "    if clause_count == 11:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "715df9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_and_modifiers(clauses):\n",
    "    \n",
    "    # obj: vanilla object\n",
    "    # pobj: object of a pr\n",
    "    # iobj: indirect obj\n",
    "    # pobj: prep. obj\n",
    "    # npadvmod: noun phrase as adverbial modifier (common for measurements; 20 years old)\n",
    "    # nsubj: noun subject\n",
    "    # nsubjpass: passive nominal subject\n",
    "    # acomp: adjectival complement\n",
    "    \n",
    "    np_labels = ['compound', 'obj','dobj','iobj','pobj','npadvmod','nsubj','nsubjpass']\n",
    "    \n",
    "    # modifiers ... acl, amod, nummod, nn, advmod\n",
    "    \n",
    "    detail_dict = {}\n",
    "    count = 0\n",
    "    for clause in clauses:\n",
    "        print(\"==============\")\n",
    "        doc = en(clause)\n",
    "        for word in doc:\n",
    "            # if word.dep_ in np_labels \n",
    "            if word.pos_ == \"NOUN\" or word.pos_ == \"VERB\":\n",
    "                detail_chunks = [] \n",
    "                size = sum(1 for dummy in word.subtree)\n",
    "                if size > 1:\n",
    "                    detail_chunk = []\n",
    "                    for descendant in word.subtree:\n",
    "                        detail_chunk.append(descendant.text)\n",
    "                    detail_chunks.append(detail_chunk)\n",
    "                if detail_chunks:\n",
    "                    print(detail_chunks)\n",
    "  #      detail_dict[count] = \"|\".join(sum(detail_chunks, []))\n",
    "  #      count += 1\n",
    "            \n",
    "  #  print(detail_dict)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "a58acd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "[['the', 'open', 'plain']]\n",
      "==============\n",
      "[['a', 'starless', 'sky']]\n",
      "==============\n",
      "[['a', 'man']]\n",
      "[['a', 'man', 'walked', 'alone', 'along', 'the', 'highway', 'from', 'marchiennes', 'to', 'montsou']]\n",
      "[['the', 'highway']]\n",
      "[['to', 'montsou']]\n",
      "==============\n",
      "[['a', 'straight', 'paved', 'road', 'ten', 'kilometres', 'in', 'length']]\n",
      "[['ten', 'kilometres', 'in', 'length']]\n",
      "==============\n",
      "[['intersecting', 'the', 'beetroot', 'fields']]\n",
      "[['the', 'beetroot', 'fields']]\n",
      "==============\n",
      "[['he', 'could', 'not', 'even', 'see', 'the', 'black', 'soil', 'before', 'him']]\n",
      "[['the', 'black', 'soil', 'before', 'him']]\n",
      "==============\n",
      "[['only', 'felt', 'the', 'immense', 'flat', 'horizon', 'by', 'the', 'gusts', 'of', 'march', 'wind']]\n",
      "[['the', 'immense', 'flat', 'horizon', 'by', 'the', 'gusts', 'of', 'march', 'wind']]\n",
      "[['the', 'gusts', 'of', 'march', 'wind']]\n",
      "[['march', 'wind']]\n",
      "==============\n",
      "[['squalls', 'as', 'strong', 'as', 'on', 'the', 'sea']]\n",
      "[['the', 'sea']]\n",
      "==============\n",
      "[['and', 'frozen', 'from', 'sweeping', 'leagues', 'of', 'marsh', 'and', 'naked', 'earth']]\n",
      "[['sweeping', 'leagues', 'of', 'marsh', 'and', 'naked', 'earth']]\n",
      "[['marsh', 'and', 'naked']]\n",
      "[['marsh', 'and', 'naked', 'earth']]\n",
      "==============\n",
      "[['no', 'tree']]\n",
      "[['no', 'tree', 'could', 'be', 'seen', 'against', 'the', 'sky']]\n",
      "[['the', 'sky']]\n",
      "==============\n",
      "[['the', 'road', 'unrolled', 'as', 'straight', 'as', 'a', 'pier', 'in', 'the', 'midst', 'of', 'the', 'blinding', 'spray', 'of', 'darkness']]\n",
      "[['a', 'pier', 'in', 'the', 'midst', 'of', 'the', 'blinding', 'spray', 'of', 'darkness']]\n",
      "[['the', 'midst', 'of', 'the', 'blinding', 'spray', 'of', 'darkness']]\n",
      "[['the', 'blinding', 'spray', 'of', 'darkness']]\n"
     ]
    }
   ],
   "source": [
    "nouns_and_modifiers(germinal_clauses[0:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda8782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
