# -*- coding: utf-8 -*-
"""RelicClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QfuohgugH_GOqW1rKN_mqpVUjD2x_2cu

### Libraries
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install --upgrade transformers==4.20.1

# modeling
from transformers import BertModel, BertTokenizer
import torch
import torch.nn as nn

# data libraries
import pandas as pd
import numpy as np

# util
from tqdm import tqdm
import random
import time
from collections import Counter

# sklearn helpers
from sklearn.metrics import accuracy_score 
from sklearn.metrics import confusion_matrix 
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score 
from sklearn.metrics import recall_score 
from sklearn.metrics import roc_auc_score 
from sklearn.metrics import average_precision_score 
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import precision_recall_fscore_support as score
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from scipy.special import softmax

# viz
import matplotlib.pyplot as plt
import seaborn as sns

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Running on {}".format(device))

"""### Import Data"""

def read_text_data(filename):
    data = []
    with open(filename) as file:
        for line in file:
            line = line.replace("\\n", "")
            line = line.replace("\n", "")
            data.append(line)
    return data

def read_class_data(filename):
    data = []
    with open(filename) as file:
        for line in file:
            line = line.replace("\n", "")
            data.append(int(line))
    return data

# read in data (v3)

# binary x
x_train_binary = read_text_data("/content/drive/MyDrive/ANLP21/relic_analysis/data/relic_v3_binary_train_x.txt")
x_test_binary = read_text_data("/content/drive/MyDrive/ANLP21/relic_analysis/data/relic_v3_binary_test_x.txt")

# detail x 
x_train_detail = read_text_data("/content/drive/MyDrive/ANLP21/relic_analysis/data/relic_v3_detail_train_x.txt")
x_test_detail = read_text_data("/content/drive/MyDrive/ANLP21/relic_analysis/data/relic_v3_detail_test_x.txt")

# binary y
y_train_binary = read_class_data("/content/drive/MyDrive/ANLP21/relic_analysis/data/relic_v3_binary_train_y.txt") 
y_test_binary = read_class_data("/content/drive/MyDrive/ANLP21/relic_analysis/data/relic_v3_binary_test_y.txt") 

# detail y
y_train_detail = read_class_data("/content/drive/MyDrive/ANLP21/relic_analysis/data/relic_v3_detail_train_y.txt") 
y_test_detail = read_class_data("/content/drive/MyDrive/ANLP21/relic_analysis/data/relic_v3_detail_test_y.txt") 

## full
x_train_full = read_text_data("/content/drive/MyDrive/ANLP21/relic_analysis/data/relic_x_train.txt")
x_val_full = read_text_data("/content/drive/MyDrive/ANLP21/relic_analysis/data/relic_x_val.txt")
x_test_full = read_text_data("/content/drive/MyDrive/ANLP21/relic_analysis/data/relic_x_test.txt")

x_train_full = x_train_full+x_test_full

y_train_full = read_text_data("/content/drive/MyDrive/ANLP21/relic_analysis/data/relic_y_train.txt")
y_val_full = read_text_data("/content/drive/MyDrive/ANLP21/relic_analysis/data/relic_y_val.txt")
y_test_full = read_text_data("/content/drive/MyDrive/ANLP21/relic_analysis/data/relic_y_test.txt")

y_train_full = y_train_full+y_test_full

y_train_full = [int(y) for y in y_train_full]

y_val_full = [int(y) for y in y_val_full]

binary_labels = {'detail': 0, 'x': 1}

detail_labels = {'metaphor': 0, 'internal character experience': 1, 'embellishing an event': 2, 'external characterization': 3, 'informative detail': 4, 'sensory detail': 5}

full_labels = {'metaphor': 0, 'x': 1, 'internal character experience': 2, 'embellishing an event': 3, 'external characterization': 4, 'informative detail': 5, 'sensory detail': 6}

len(x_train_binary)

"""### Model Metrics"""

# calculates accuracy of predictions
# used to track efficacy over each epoch
def evaluate(model, x, y):
    model.eval()
    corr = 0.
    total = 0.
    with torch.no_grad():
        for x, y in zip(x, y):
            y_preds=model.forward(x)
            for idx, y_pred in enumerate(y_preds):
                prediction=torch.argmax(y_pred)
                if prediction == y[idx]:
                    corr += 1.
                total+=1                          
    return corr/total

# calculates:
# accuracy, precision, recall and f1
# utilized after a model is trained
def metric_gatherer(model, x,  y, pred_labels, multi_class):
    model.eval()
    preds = []
    scores = []
    probs = []

    with torch.no_grad():
        for x, y in zip(x, y):
            y_preds=model.forward(x)
            for idx, y_pred in enumerate(y_preds.cpu().numpy()):
                prediction = np.argmax(y_pred)
                probs.append(softmax(y_pred)[0])
                preds.append(prediction)
    y = list(y.cpu().numpy())
    
    # accuracy
    scores.append(accuracy_score(y, preds))
    # macro averaged precision
    scores.append(precision_score(y, preds, average = 'macro'))
    # macro averaged recall
    scores.append(recall_score(y, preds, average = 'macro'))
    # macro averaged f1
    scores.append(f1_score(y, preds, average = 'macro'))
    if not multi_class:
      # aucRoc
      scores.append(roc_auc_score(y, probs))
      # pr AUC
      scores.append(average_precision_score(y, probs))
      #calculate precision and recall
      precision, recall, thresholds = precision_recall_curve(y, preds)

      #create precision recall curve
      fig, ax = plt.subplots()
      ax.plot(recall, precision, color='purple')

      #add axis labels to plot
      ax.set_title('Precision-Recall Curve')
      ax.set_ylabel('Precision')
      ax.set_xlabel('Recall')

      #display plot
      plt.show()

      return scores
    else:
      s = score(y, preds, labels=list(full_labels.values()))
      
# It is possible to compute per-label precisions, recalls, F1-scores and supports instead of averaging:
      for i in range(len(s)):
        if i == 0:
          print(f"precisions --\n {list(s[i])}")
        elif i == 1:
          print(f"recalls -----\n {list(s[i])}")
        elif i == 2:
          print(f"F1-scores ---\n {list(s[i])}")
        else:
          print(f"supports ----\n {list(s[i])}")
      
      return 0

# creates a confusion matrix
def create_confusion_matrix(model, x, y, labels):
    model.eval()
    preds = []
    scores = []

    with torch.no_grad():
        for x, y in zip(x, y):
            y_preds=model.forward(x)
            for idx, y_pred in enumerate(y_preds.cpu().numpy()):
                prediction = np.argmax(y_pred)
                preds.append(prediction)
    
    y = np.array(y.cpu())
    y_true = list(y)

    plt.figure(figsize = (18,8))
    ax = sns.heatmap(confusion_matrix(y_true, preds), annot = True, xticklabels = list(labels.keys()), yticklabels = list(labels.keys()), cmap = 'summer',  fmt='d')
    # ax.invert_yaxis()
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

# outputs true labels and predicted labels as lists
def get_raw_preds(model, x, y):
    model.eval()
    preds = []
    scores = []

    with torch.no_grad():
        for x, y in zip(x, y):
            y_preds=model.forward(x)
            for idx, y_pred in enumerate(y_preds.cpu().numpy()):
                prediction = np.argmax(y_pred)
                preds.append(prediction)
    
    y = np.array(y.cpu())
    y_true = list(y)

    return y_true, preds

list(full_labels.values())

"""### Bert Classifier Data Model (tweaked for testing unseen samples)"""

# Python class for BERTClassifier
class BERTClassifierForTesting(nn.Module):

    def __init__(self, params):
        # inherit from PreTrainedModel https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel
        super().__init__()

        self.model_name=params["model_name"]
        self.base_model_name=params["base_model"]
        self.tokenizer = BertTokenizer.from_pretrained(self.base_model_name, do_lower_case=params["doLowerCase"], do_basic_tokenize=False)
        self.bert = BertModel.from_pretrained(self.base_model_name)
  
        # designate number of labels
        self.num_labels = params["label_length"]
        # attach embedding dimensions based on particular model
        self.fc = nn.Linear(params["embedding_size"], self.num_labels)

    # batch processing for real-world (ie unlabeled) data
    def get_testing_batches(self, testing_x, batch_size):
      
      batches_x = []
      for i in range(0, len(testing_x), batch_size):
        
        # get tranche
        current_batch = []
        x = testing_x[i: i+batch_size]

        # call tokenizer
        batch_x = self.tokenizer(x, padding='max_length', truncation=False, return_tensors="pt", max_length=512)
        batches_x.append(batch_x.to(device))
    
      return batches_x
  
    # define steps for forward pass
    def forward(self, batch_x):
    
        bert_output = self.bert(input_ids=batch_x["input_ids"],
                         attention_mask=batch_x["attention_mask"],
                         token_type_ids=batch_x["token_type_ids"],
                         output_hidden_states=True)
        
        bert_hidden_states = bert_output['hidden_states']

        # We're going to represent an entire document just by its [CLS] embedding (at position 0)
        out = bert_hidden_states[-1][:,0,:]

        out = self.fc(out)

        return out.squeeze()

"""### BERT Classifier Data Model"""

# Python class for BERTClassifier
class BERTClassifier(nn.Module):

    def __init__(self, params):
        # inherit from PreTrainedModel https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel
        super().__init__()

        self.model_name=params["model_name"]
        self.tokenizer = BertTokenizer.from_pretrained(self.model_name, do_lower_case=params["doLowerCase"], do_basic_tokenize=False)
        self.bert = BertModel.from_pretrained(self.model_name)

        # designate number of labels
        self.num_labels = params["label_length"]
        # attach embedding dimensions based on particular model
        self.fc = nn.Linear(params["embedding_size"], self.num_labels)

    # batch processing of input data through the model
    def get_batches(self, all_x, all_y, batch_size=32, max_toks=512):
            
        """ Get batches for input x, y data, with data tokenized according to the BERT tokenizer 
      (and limited to a maximum number of WordPiece tokens """

        batches_x=[]
        batches_y=[]
        
        for i in range(0, len(all_x), batch_size):

            current_batch=[]

            x=all_x[i:i+batch_size]

            # call tokenizer
            batch_x = self.tokenizer(x, padding='max_length', truncation=False, return_tensors="pt", max_length=max_toks)
            batch_y=all_y[i:i+batch_size]

            batches_x.append(batch_x.to(device))
            batches_y.append(torch.LongTensor(batch_y).to(device))
            
        return batches_x, batches_y
  
    # define steps for forward pass
    def forward(self, batch_x): 
    
        bert_output = self.bert(input_ids=batch_x["input_ids"],
                         attention_mask=batch_x["attention_mask"],
                         token_type_ids=batch_x["token_type_ids"],
                         output_hidden_states=True)

        bert_hidden_states = bert_output['hidden_states']

        # We're going to represent an entire document just by its [CLS] embedding (at position 0)
        out = bert_hidden_states[-1][:,0,:]

        out = self.fc(out)

        return out.squeeze()

# helper function to identify the missing values in a list of numbers
def find_missing(values, min, max):
    if min not in values:
        return [min]
    if max not in values:
        return [max]
    values = sorted(values)
    return sorted(set(range(values[0], values[-1])) - set(values))

def train_and_evaluate(bert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels, embedding_size=768, doLowerCase=None, rebalance_weights=True):

  start_time=time.time()
  bert_model = BERTClassifier(params={"doLowerCase": doLowerCase, "model_name": bert_model_name, "embedding_size":embedding_size, "label_length": len(labels)})
  bert_model.to(device)
  
  # these are lists of 'batched' tensors
  # split into blocks of 32
  batch_x, batch_y = bert_model.get_batches(train_x, train_y)
  dev_batch_x, dev_batch_y = bert_model.get_batches(dev_x, dev_y)

  optimizer = torch.optim.Adam(bert_model.parameters(), lr=1e-5)

  num_epochs=5
  best_dev_acc = 0.

  for epoch in range(num_epochs):
      bert_model.train()

      # Train
      for x, y in tqdm(list(zip(batch_x, batch_y))):
          y_pred = bert_model.forward(x)
          # instantiate vanilla loss function
          cross_entropy=nn.CrossEntropyLoss()
          # rebalance if desired
          if rebalance_weights:
            # calculate the class proportions, of this batch
            class_weights = compute_class_weight('balanced', classes=np.unique(np.array(y.cpu())), y = np.array(y.cpu()))
            # ensure that every class is represented in the class_weights array
            if len(class_weights) < bert_model.num_labels:
                # print("fixing labels")
                labels = list(np.array(y.cpu()))
                to_add = find_missing(labels, 0, bert_model.num_labels-1)
                # insert 0's for labels not in the weights list
                for missing in to_add:
                    class_weights = np.insert(class_weights, missing, 0)
            # convert to FloatTensor and store on gpu
            class_weights_torch = torch.from_numpy(class_weights).float().to(device)
            # inject that information into CrossEntropyLoss
            cross_entropy = nn.CrossEntropyLoss(weight=class_weights_torch, reduction='mean')
          # assign cross_entropy func
          loss = cross_entropy(y_pred.view(-1, bert_model.num_labels), y.view(-1))
          # clear out prior gradients
          optimizer.zero_grad()
          # invoke loss
          loss.backward()
          # parameter updates
          optimizer.step()
      
      # Evaluate
      dev_accuracy=evaluate(bert_model, dev_batch_x, dev_batch_y)
      if epoch % 1 == 0:
          print("Epoch %s, dev accuracy: %.3f" % (epoch, dev_accuracy))
          if dev_accuracy > best_dev_acc:
              torch.save(bert_model.state_dict(), model_filename)
              best_dev_acc = dev_accuracy

  bert_model.load_state_dict(torch.load(model_filename))
  torch.save(bert_model.state_dict(), "/content/drive/MyDrive/ANLP21/relic_analysis/models/" + model_filename + ".pt")

  print("\nBest Performing Model achieves dev accuracy of : %.3f" % (best_dev_acc))
  print("Time: %.3f seconds ---" % (time.time() - start_time))

  return bert_model

def model_scoring(model, val_x, val_y, labels, multi_class):
    metrics_dev_x, metrics_dev_y = model.get_batches(val_x, val_y, batch_size=len(val_x))
    metrics = metric_gatherer(model, metrics_dev_x, metrics_dev_y, labels, multi_class)
    
    if not multi_class:
      print(f"accuracy: {metrics[0]}\nrecall: {metrics[1]}\nprecision: {metrics[2]}\nF1: {metrics[3]}\nROC AUC: {metrics[4]}\npr_AUC ROC: {metrics[5]}")
    else:
      print("DONE!")
    # construct confusion matrix
    create_confusion_matrix(model, metrics_dev_x, metrics_dev_y, labels)

    # get raw preds
    y_true, y_pred = get_raw_preds(model, metrics_dev_x, metrics_dev_y) 

    return y_true, y_pred

"""### BERT Classifiers

all the data is used for each model rather than making partial splits.

ensemble-ish except they run separately on different tasks so they can use the same data; more of a phase-1, phase-2 setup.

#### BERT Base

#### Binary (v3) -- sending everything in dataset to 0 (detail) or 1 (not detail)
"""

bert_base_v3_binary = train_and_evaluate("google/bert_uncased_L-12_H-768_A-12", "relic-bert-base-uncased_v3_binary", x_train_binary, y_train_binary, x_test_binary, y_test_binary, binary_labels, embedding_size=768, doLowerCase=True, rebalance_weights=False)

bert_base_v3_binary_true, bert_base_v3_binary_preds = model_scoring(bert_base_v3_binary, x_test_binary, y_test_binary, binary_labels)

"""#### Detail model (v3) - as big as possible dataset for mulit-class w/o 'junk' cat"""

detail_labels

bert_base_v3_detail = train_and_evaluate("google/bert_uncased_L-12_H-768_A-12", "relic-bert-base-uncased_v3_detail", x_train_detail, y_train_detail, x_test_detail, y_test_detail, detail_labels, embedding_size=768, doLowerCase=True, rebalance_weights=False)

"""#### Full Model (v3) - complete task, including 'x' category."""

bert_base_v3_full = train_and_evaluate("google/bert_uncased_L-12_H-768_A-12", "relic-bert-base-uncased_v3_full", x_train_full, y_train_full, x_val_full, y_val_full, full_labels, embedding_size=768, doLowerCase=True, rebalance_weights=False)

full_labels

bert_base_v3_full_true, bert_base_v3_full_preds = model_scoring(bert_base_v3_full, x_val_full, y_val_full, full_labels, True)

"""### Testing on Full novels"""

# read in great gatbsy paragraphs
great_gatsby_df = pd.read_csv("/content/drive/MyDrive/ANLP21/relic_analysis/novels/the_great_gatsby_paragraphs.csv", index_col=0)

great_gatsby_df.head()

# remove first row
great_gatsby_df = great_gatsby_df[1:]

# invokes the bert_model's tokenizer to parse the incoming paragraph
def tokenize_paragraphs(paragraphs, tokenizer):

    tokenized_text = []
    for p in paragraphs:
      sample_toks = tokenizer.tokenize(p)
      text = tokenizer.convert_tokens_to_string(sample_toks)
      tokenized_text.append(text)
    
    return tokenized_text

base_model = "google/bert_uncased_L-12_H-768_A-12"
full_model_name="/content/drive/MyDrive/ANLP21/relic_analysis/models/relic-bert-base-uncased_v3_full.pt"

full_model = BERTClassifierForTesting(params={"base_model": base_model, "doLowerCase": True, "model_name": full_model_name, "embedding_size":768, "label_length": len(full_labels)})
full_model.load_state_dict(torch.load(full_model_name))
full_model = full_model.to(device)

# tokenize text
tokenized_paragraphs = tokenize_paragraphs(great_gatsby_df['text'], full_model.tokenizer)

# batch text -- adjusted to batch size of 30
paragraphs_batched = full_model.get_testing_batches(tokenized_paragraphs, 30)

len(paragraphs_batched)

def classify_unseen_paragraphs(x, model):

    # set model to eval mode
    model.eval()
    _, maxlen=x["input_ids"].shape
    
    # baseline is uninformative sequence of padding tokens
    baseline=torch.LongTensor([[0]*maxlen]).to(device)

    # invoke model forward pass
    y_preds=model.forward(x)
    
    # extract information
    y_preds=torch.nn.functional.softmax(y_preds, dim=1)
    y_preds=y_preds.cpu().data.numpy()

    # bucket for returning predictions
    preds=[]

    for y_pred in y_preds:
        prediction=np.argmax(y_pred)
        preds.append(prediction)
    
    return preds

preds = []
for i in tqdm(range(len(paragraphs_batched))):
    preds.append(classify_unseen_paragraphs(paragraphs_batched[i], full_model))

results = [item for sublist in preds for item in sublist]

great_gatsby_df['predicted_labels'] = results

great_gatsby_df.head()

great_gatsby_df.to_csv('/content/drive/MyDrive/ANLP21/relic_analysis/novels/the_great_gatsby_with_predictions.csv')

