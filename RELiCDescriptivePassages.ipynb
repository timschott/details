{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b311692",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "\n",
    "Explore dataset from [RELiC](https://relic.cs.umass.edu/), a novel information retrieval task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe87bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb69bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Read in .json of RELiC data as a nested dict.\n",
    "Data structured as follows:\n",
    "\n",
    "{\n",
    "    book_n: {\n",
    "        quotes: {\n",
    "            quote_id: [\n",
    "                [quote_n_left] # 4 sentences, left 'context' of critical claim\n",
    "                sentence_id # index of the corresponding sentence in `sentences`\n",
    "                sentence_window_size # number from 1-5 =to how many original sentences inhere the claim\n",
    "                [quote_n_right] # 4 sentences, right 'context' of critical claim\n",
    "            ], ...\n",
    "        },\n",
    "        sentences: [sentence_1, sentence_2], # all the sentences in the work\n",
    "        candidates: { # tracks which sentences are eligible to be 'expanded' e.g. if sentence #7 is used in a claim that's 3 sentences long, we should retrieve sentences 7,8,9.\n",
    "            1_sentence: [all sentence_id's],\n",
    "            ...,\n",
    "            5-sentence: [all but last 4 sentence_ids (prevent OOB error)], \n",
    "        }\n",
    "    },\n",
    "}\n",
    "'''\n",
    "def read_data(filename):\n",
    "    \n",
    "    f = open(filename)\n",
    "    data = json.load(f)\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da8d1e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data('data/relic-train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d4929ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'brothers_karamazov')\n",
      "(1, 'to_the_lighthouse')\n",
      "(2, 'the_pickwick_papers')\n",
      "(3, 'david_copperfield')\n",
      "(4, 'animal_farm')\n",
      "(5, 'the_scarlet_letter')\n",
      "(6, 'a_portrait_of_the_artist_as_a_young_man')\n",
      "(7, 'the_turn_of_the_screw')\n",
      "(8, 'the_souls_of_black_folk')\n",
      "(9, 'adam_bede')\n",
      "(10, 'sense_and_sensibility')\n",
      "(11, 'martin_chuzzlewit')\n",
      "(12, 'swanns_way')\n",
      "(13, 'sister_carrie')\n",
      "(14, 'daisy_miller')\n",
      "(15, 'o_pioneers')\n",
      "(16, 'the_red_badge_of_courage')\n",
      "(17, 'little_dorrit')\n",
      "(18, 'great_expectations')\n",
      "(19, 'the_call_of_the_wild')\n",
      "(20, 'mrs_dalloway')\n",
      "(21, 'the_sport_of_the_gods')\n",
      "(22, 'middlemarch')\n",
      "(23, 'alices_adventures_in_wonderland')\n",
      "(24, 'jacobs_room')\n",
      "(25, '1984')\n",
      "(26, 'house_of_mirth')\n",
      "(27, 'nicholas_nickleby')\n",
      "(28, 'moby_dick')\n",
      "(29, 'oliver_twist')\n",
      "(30, 'jane_eyre')\n",
      "(31, 'this_side_of_paradise')\n",
      "(32, 'madame_bovary')\n",
      "(33, 'iola_leroy')\n",
      "(34, 'frankenstein')\n",
      "(35, 'the_age_of_innocence')\n",
      "(36, 'lady_chatterlys_lover')\n",
      "(37, 'maggie_a_girl_of_the_streets')\n",
      "(38, 'wuthering_heights')\n",
      "(39, 'the_ambassadors')\n",
      "(40, 'brave_new_world')\n",
      "(41, 'an_american_tragedy')\n",
      "(42, 'adventures_of_huckleberry_finn')\n",
      "(43, 'gone_with_the_wind')\n",
      "(44, 'the_awakening')\n",
      "(45, 'mansfield_park')\n",
      "(46, 'the_waves')\n",
      "(47, 'uncle_toms_cabin')\n",
      "(48, 'my_antonia')\n",
      "(49, 'the_rainbow')\n",
      "(50, 'bleak_house')\n",
      "(51, 'crime_and_punishment')\n",
      "(52, 'the_picture_of_dorian_gray')\n",
      "(53, 'a_christmas_carol')\n",
      "(54, 'war_and_peace')\n",
      "(55, 'what_maisie_knew')\n"
     ]
    }
   ],
   "source": [
    "# what books are in the dataset:\n",
    "for i in enumerate(data):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd2b1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Read in .json of RELiC data as a nested dict.\n",
    "Data structured as follows:\n",
    "\n",
    "{\n",
    "    book_n: {\n",
    "        quotes: { \n",
    "            quote_n: [\n",
    "                [quote_n_left] # 4 sentences, left 'context' of critical claim\n",
    "                sentence_id # index of the corresponding sentence in `sentences`\n",
    "                sentence_window_size # number from 1-5 =to how many original sentences inhere the claim\n",
    "                [quote_n_right] # 4 sentences, right 'context' of critical claim\n",
    "            ], ...\n",
    "        },\n",
    "        sentences: [sentence_1, sentence_2], # all the sentences in the work\n",
    "        candidates: { # tracks which sentences are eligible to be 'expanded' e.g. if sentence #7 is used in a claim that's 3 sentences long, we should retrieve sentences 7,8,9.\n",
    "            1_sentence: [all sentence_id's],\n",
    "            ...,\n",
    "            5-sentence: [all but last 4 sentence_ids (prevent OOB error)], \n",
    "        }\n",
    "    },\n",
    "}\n",
    "'''\n",
    "\n",
    "'''\n",
    "for a given work in the dataset, extract passages deemed by critics to be \"descriptive\"\n",
    "'''\n",
    "def extract_passages(data, book_title, search_list):\n",
    "    # get associated data for a single title\n",
    "    book_data = data[book_title]\n",
    "    \n",
    "    quotes = book_data[\"quotes\"]\n",
    "    sentences = book_data[\"sentences\"]\n",
    "    \n",
    "    descriptive_sentences = {}\n",
    "    descriptive_count = 0\n",
    "    claim_count = 0\n",
    "    \n",
    "    for quote_id in quotes:\n",
    "        left_claim = ' '.join(quotes[quote_id][0]).lower()\n",
    "        sentence_id = quotes[quote_id][1]\n",
    "        quote_size = quotes[quote_id][2]\n",
    "        right_claim = ' '.join(quotes[quote_id][3]).lower()\n",
    "                \n",
    "        descriptive = any(substring in left_claim for substring in search_list) or any(substring in right_claim for substring in search_list)\n",
    "        \n",
    "        if descriptive:\n",
    "            descriptive_sentences[quote_id] = sentences[sentence_id: sentence_id+quote_size]\n",
    "            descriptive_count +=1\n",
    "        \n",
    "        claim_count +=1\n",
    "\n",
    "    print(f'Found {descriptive_count} out of {claim_count} passages in {book_title}')\n",
    "    return descriptive_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "424f8de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 215 out of 1831 passages in to_the_lighthouse\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_passages(data, 'to_the_lighthouse', ['descri', 'detail', 'zoom in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a1b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
