{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "20588c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "import sys\n",
    "# reversing a dict\n",
    "import operator\n",
    "# math work\n",
    "import math \n",
    "# for counts\n",
    "from collections import Counter\n",
    "# tokenizing \n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "753a3a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reads a new-line delimited file representing a \"corpus\" for a given category, group etc.\n",
    "Tokenizes to lowercase, strip spaces and remove non-alphanumeric tokens w/ `nltk`.\n",
    "This tokenization strategy makes sense because these aren't the people-facing samples, these are for the comp. pipeline\n",
    "\n",
    "Args:\n",
    "    filename: location of file\n",
    "\n",
    "Return:\n",
    "    tokens: a list of the tokens found in the file\n",
    "'''\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def read_and_tokenize(filename):\n",
    "    \n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        tokens=[]\n",
    "        # lowercase\n",
    "        for line in file:\n",
    "            # remove spaces, lowercase\n",
    "            data=line.rstrip().lower()\n",
    "            # tokenize, getting rid of any non alphanumeric chars\n",
    "            tokens.extend(tokenizer.tokenize(data))\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f00df75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Provides a dictionary with frequency counts for an inputted `tokens` list.\n",
    "Via the `Counter` api.\n",
    "\n",
    "Args:\n",
    "    tokens: tokens from a corpus\n",
    "\n",
    "Return:\n",
    "    counts: associative mapping between a word and its frequency\n",
    "'''\n",
    "def get_counts(tokens):\n",
    "    counts=Counter()\n",
    "    for token in tokens:\n",
    "        counts[token]+=1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e4afaf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Similar to `get_counts`, but tallies a frequency score for each word used in a file.\n",
    "In this case, I'm going to hand off one big text file that contains every work in the corpus.\n",
    "\n",
    "Args:\n",
    "    filename: location of corpus file\n",
    "\n",
    "Return:\n",
    "    freqs: mapping between each word and its relative frequency\n",
    "'''\n",
    "def read_priors(filename):\n",
    "    counts=Counter()\n",
    "    freqs={}\n",
    "    tokens=read_and_tokenize(filename)\n",
    "    total=len(tokens)\n",
    "\n",
    "    for token in tokens:\n",
    "        counts[token]+=1\n",
    "\n",
    "    for word in counts:\n",
    "        freqs[word]=counts[word]/total\n",
    "\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8892e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Similar to `get_counts`, but tallies a frequency score for each word in a list\n",
    "\n",
    "Args:\n",
    "    token_list: a list of words\n",
    "\n",
    "Return:\n",
    "    counts: mapping between each word and its frequency\n",
    "'''\n",
    "def count_combined_list(token_list):\n",
    "    counts=Counter()\n",
    "\n",
    "    for token in token_list:\n",
    "        counts[token]+=1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "531022ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Math helper for log-odds ratio \n",
    "Args:\n",
    "    y_i_w: count of a word in i\n",
    "    y_j_w: count of a word in j\n",
    "    alpha_w: the bias term\n",
    "    alpha_not: the derived alpha_not (\"informative\" prior)\n",
    "    n_i: number of words in i\n",
    "    n_j: number of words in j\n",
    "    debug: boolean flag - setting to True will output information about the calculation\n",
    "    \n",
    "Return: \n",
    "    result: the computed difference \n",
    "'''\n",
    "def math_helper(y_i_w, y_j_w, alpha_w, alpha_not, n_i, n_j, debug):\n",
    "    \n",
    "    ## first, log-odds\n",
    "    y_i_log = math.log2((y_i_w + alpha_w) / (n_i + alpha_not - y_i_w - alpha_w))\n",
    "    \n",
    "    y_j_log = math.log2((y_j_w + alpha_w) / (n_j + alpha_not - y_j_w - alpha_w))\n",
    "    \n",
    "    log_odds = y_i_log - y_j_log\n",
    "    \n",
    "    ## next, variance\n",
    "    y_i_variance = 1 / (y_i_w + alpha_w)\n",
    "    y_j_variance = 1 / (y_j_w + alpha_w)\n",
    "    \n",
    "    variance = y_i_variance + y_j_variance\n",
    "    \n",
    "    result = log_odds / (math.sqrt(variance))\n",
    "    \n",
    "    if (debug == True):\n",
    "        print(\"alpha_w = {}, log_odds={}, variance={}, result={}\".format(alpha_w, log_odds, variance, result))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96b2a5",
   "metadata": {},
   "source": [
    "Logg odds, $\\hat\\zeta_w^{(i-j)}$ for word $w$ reflecting the difference in usage between corpus $i$ and corpus $j$, is given by the following equation:\n",
    "\n",
    "$$\n",
    "\\hat\\zeta_w^{(i-j)}= {\\hat{d}_w^{(i-j)} \\over \\sqrt{\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right)}}\n",
    "$$\n",
    "\n",
    "Where: \n",
    "\n",
    "$$\n",
    "\\hat{d}_w^{(i-j)} = \\log \\left({y_w^i + \\alpha_w} \\over {n^i + \\alpha_0 - y_w^i - \\alpha_w}) \\right) -  \\log \\left({y_w^j + \\alpha_w} \\over {n^j + \\alpha_0 - y_w^j - \\alpha_w}) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right) \\approx {1 \\over {y_w^i + \\alpha_w}} + {1 \\over {y_w^j + \\alpha_w} }\n",
    "$$\n",
    "\n",
    "And:\n",
    "\n",
    "* $y_w^i = $ count of word $w$ in corpus $i$ (likewise for $j$)\n",
    "* $\\alpha_w$ = 0.01\n",
    "* $V$ = size of vocabulary (number of distinct word types)\n",
    "* $\\alpha_0 = V * \\alpha_w$\n",
    "* $n^i = $ number of words in corpus $i$ (likewise for $j$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "11690e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Implements the log-odds ratio with an informative Dirichlet prior \n",
    "(Described in Monroe et al. 2009, Fighting Words) \n",
    "Args:\n",
    "    one_tokens: tokens from first label\n",
    "    one_counts: frequencies from first label\n",
    "    two_tokens: tokens from second label\n",
    "    two_counts: frequencies from second label\n",
    "    priors: relative frequencies from \"joint\" corpus - (\"informative\")\n",
    "    \n",
    "Return: \n",
    "    result: log_odds_dict, a mapping between a word and its log-odds\n",
    "        the more positive a value, the more it is aligned with the \"detail\" group\n",
    "\n",
    "'''\n",
    "def logodds_with_informative_prior(one_tokens, one_counts, two_tokens, two_counts, priors):\n",
    "     \n",
    "    ## create a list combining the two corpora\n",
    "    ## this is what we will iterate through\n",
    "    combined_tokens = list(set(one_tokens + two_tokens))\n",
    "    \n",
    "    ## alpha_not = size of vocab * .01\n",
    "    alpha_not = len(combined_tokens) * .01\n",
    "    \n",
    "    ## next, calculate corpus sizes\n",
    "    detail_n = len(one_tokens)\n",
    "    not_detail_n = len(two_tokens)\n",
    "    \n",
    "    ## with the detail token in the first log-chunk, the more positive a word is, the more aligned\n",
    "    ## it is with the detail corpus\n",
    "    ## w/ 0 as the marking line\n",
    "    ## so, once calculation is complete, sort the list and pull out the 25 largest and 25 smallest vals\n",
    "    \n",
    "    log_odds_dict = {}\n",
    "    for word in combined_tokens:\n",
    "        ## skip words fragments that may appear at beginning/end of samples        \n",
    "        if word not in priors:\n",
    "            continue\n",
    "        \n",
    "        ## count of word in detail corpus\n",
    "        detail_y_w = one_counts[word]\n",
    "        \n",
    "        ## count of word in not_detail corpus\n",
    "        not_detail_y_w = two_counts[word]\n",
    "        \n",
    "        ## my prior -- tamp down very common words like \"the\"\n",
    "        prior_w = priors[word]\n",
    "        \n",
    "        ## calculate alpha_w on the fly, now\n",
    "        alpha_w = alpha_not * prior_w\n",
    "        \n",
    "        ## carry out log odds calculation\n",
    "        result = math_helper(detail_y_w, not_detail_y_w, alpha_w, alpha_not, detail_n, not_detail_n, False)\n",
    "        \n",
    "        ## commit to results dict\n",
    "        log_odds_dict[word] = result\n",
    "        \n",
    "    return log_odds_dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b3450e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "detail_tokens=read_and_tokenize(\"detail_samples.txt\")\n",
    "not_detail_tokens=read_and_tokenize(\"not_detail_samples.txt\")\n",
    "\n",
    "detail_counts = get_counts(detail_tokens)\n",
    "not_detail_counts = get_counts(not_detail_tokens)\n",
    "\n",
    "priors = read_priors(\"../Gutenberg/merged.txt\")\n",
    "\n",
    "log_odds_dict_with_informative_prior = logodds_with_informative_prior(detail_tokens, detail_counts, not_detail_tokens, not_detail_counts, priors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "44df896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_log_odds_dict_with_informative_prior = sorted(log_odds_dict_with_informative_prior.items(), key=operator.itemgetter(1), reverse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f5f81862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 most detail words ----\n",
      "[('the', 15.047004638994647), ('of', 6.382370326356031), ('with', 6.113952460694612), ('house', 5.554706651189872), ('and', 5.1069088753192435), ('up', 5.016458242350711), ('three', 4.863483962709612), ('a', 4.543889821797246), ('had', 4.469231328708773), ('two', 4.430870927364034), ('room', 4.089514752973798), ('were', 3.9870066804149067), ('black', 3.97569210497496), ('along', 3.83313004443763), ('in', 3.805744500913115), ('through', 3.710348829252746), ('place', 3.6879761380938536), ('behind', 3.679455671321996), ('light', 3.637921579207515), ('gatsby', 3.5692903011290107)]\n",
      "25 most not_detail words ----\n",
      "[('if', -5.341157067146428), ('what', -5.4478787757761475), ('may', -5.5282946033548805), ('don', -5.646696288686094), ('am', -5.718516357068141), ('tell', -5.782383687979141), ('that', -6.028068069800341), ('not', -6.202648307398496), ('are', -6.272452265447527), ('your', -6.552078491784141), ('should', -6.80156680007829), ('do', -6.891941558401004), ('know', -7.247065490879481), ('t', -7.353073624221467), ('can', -7.374613477849019), ('have', -8.8257006250124), ('i', -9.135723848626684), ('be', -9.445730142961402), ('is', -9.549073235625025), ('you', -16.251896394179017)]\n"
     ]
    }
   ],
   "source": [
    "print(\"25 most detail words ----\")\n",
    "print([tuple for tuple in sorted_log_odds_dict_with_informative_prior[:20]])\n",
    "print(\"25 most not_detail words ----\")\n",
    "print([tuple for tuple in sorted_log_odds_dict_with_informative_prior[-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "577c1642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Graph the top 20 per category.\n",
    "combined_tokens = detail_tokens + not_detail_tokens\n",
    "combined_counts = count_combined_list(combined_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a846af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip = []\n",
    "for word, score in log_odds_dict_with_informative_prior.items():\n",
    "    container = []\n",
    "    container.append(word)\n",
    "    container.append(score)\n",
    "    container.append(combined_counts[word])\n",
    "    trip.append(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0a4bc4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1b441f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={0: 'word', 1: 'log_odds', 2: 'frequency'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "59086336",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('log_odds', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "eaecfbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>log_odds</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3168</th>\n",
       "      <td>the</td>\n",
       "      <td>15.047005</td>\n",
       "      <td>2941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3181</th>\n",
       "      <td>of</td>\n",
       "      <td>6.382370</td>\n",
       "      <td>1588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5061</th>\n",
       "      <td>with</td>\n",
       "      <td>6.113952</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word   log_odds  frequency\n",
       "3168   the  15.047005       2941\n",
       "3181    of   6.382370       1588\n",
       "5061  with   6.113952        453"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6423d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out as csv\n",
    "df.to_csv(\"log_odds_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
